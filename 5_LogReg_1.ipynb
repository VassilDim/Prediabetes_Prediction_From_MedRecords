{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510d0889",
   "metadata": {},
   "source": [
    "# 5. Logistic Regression (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccacf4c8",
   "metadata": {},
   "source": [
    "**RECAP:** From **4. Test/Train Split and Exploratory Data Analysis** we have a training data ready for logistic regression.  \n",
    "  \n",
    "The current goal will be to perform a logistic regression on a subset of the train data in order to identify features that do not contribute to explaning/predicting the target variable (prediabetes status). This information will justify removing these features in order to create space to add features from the *observations* table that was so far not included in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348e6da",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576bec2",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab81d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbae5d",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e95edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv ('X_train.csv', index_col = 0)\n",
    "y_train = pd.read_csv ('y_train.csv', index_col = 0)\n",
    "y_train = y_train['prediabetes_bin_y'].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3abce",
   "metadata": {},
   "source": [
    "### Use only 33% of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83243d93",
   "metadata": {},
   "source": [
    "Only 33% of the training data will be used to optimise the hyperparameters for logistic regression. Then, the full training dataset will be used to create another train/test (validation) split in order to assess the performance of the model and obtain coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59773568",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_33, _, y_train_33, _ = train_test_split (X_train, y_train, test_size=0.67, stratify=y_train)\n",
    "\n",
    "del X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99f76a",
   "metadata": {},
   "source": [
    "## Optimise hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060504f",
   "metadata": {},
   "source": [
    "A pipeline is constructed to scale the data and test various options for pre-processing and logistic regression parameters such as:\n",
    "- dimensionality reduction through PCA\n",
    "- regularisation\n",
    "    - Ridge\n",
    "    - Lasso\n",
    "- penalty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545137c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pre-processing and  hyperparameter options\n",
    "scaling_options = [MinMaxScaler()] # Scaling\n",
    "pca_options = [None, 5] # Dimensionality reduction options\n",
    "penalty_options = ['l1', 'l2'] # Lasso or Ridge regression\n",
    "C_options = [0.01, 0.1, 0, 1, 10] # Coefficient penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d98804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a pipeline without any parameters:\n",
    "pipe = Pipeline (\n",
    "    [\n",
    "        ('scaling', MinMaxScaler),\n",
    "        ('pca', PCA(n_components = 2)),\n",
    "        ('logreg', LogisticRegression(max_iter = 500))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d738df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for grid search\n",
    "param_grid = [\n",
    "    {\n",
    "        'scaling' : scaling_options,\n",
    "        'pca' : pca_options,\n",
    "        'logreg__penalty' : penalty_options,\n",
    "        'logreg__C' : C_options\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7990bc",
   "metadata": {},
   "source": [
    "A grid search is performed using the 33% of the training dataset defined above and a 10-fold cross-validation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc05195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search with the pipeline and hyperparameters:\n",
    "grid_search = GridSearchCV (pipe, # pipeline initiated\n",
    "                            param_grid = param_grid, # grid parameter options\n",
    "                            cv = 10) # Use cross-validation of 10-fold\n",
    "\n",
    "# Fit the grid search for best logistic regression parameter on train data\n",
    "grid_search.fit (X_train_33, y_train_33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d22122",
   "metadata": {},
   "source": [
    "Identify the best parameters for logistic regression based on 33% of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81550527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model:\n",
    "best_model = grid_search.best_estimator_\n",
    "# Extract the best hyperparameters:\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\\n\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9422dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up:\n",
    "del X_train_33, y_train_33, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11178309",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf697b0",
   "metadata": {},
   "source": [
    "Logistic regression will be performed on all the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ad08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data again:\n",
    "X_train = pd.read_csv ('X_train.csv', index_col = 0)\n",
    "y_train = pd.read_csv ('y_train.csv', index_col = 0)\n",
    "y_train = y_train['prediabetes_bin_y'].squeeze()\n",
    "\n",
    "## Train a logistic regression with the best parameters discovered:\n",
    "# 1. Scale data using\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform (X_train)\n",
    "# 2. Instantiate Logistic Regression\n",
    "logreg = LogisticRegression (penalty = 'l2',\n",
    "                             C = 10,\n",
    "                             max_iter = 1000) # Increase the maximum number of iterations\n",
    "# 3. Fit Logistic Regression on the scaled train dataset\n",
    "logreg.fit (X_train_scaled, y_train)\n",
    "\n",
    "# Predict rating for train:\n",
    "lg_best_train = logreg.predict (X_train_scaled)\n",
    "# Training performance:\n",
    "report_train = classification_report (y_train, lg_best_train)\n",
    "print(report_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817a130",
   "metadata": {},
   "source": [
    "The logistic regression did not perform very well. Despite this, information from this model will be used to further clean and modify the data in order to increase performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d06c21",
   "metadata": {},
   "source": [
    "## Coefficient importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f291fb0",
   "metadata": {},
   "source": [
    "Coefficients will be extracted and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain a measure of feature importance:\n",
    "\n",
    "# Write into a dataframe:\n",
    "coef_df = pd.DataFrame ({'Features' : ['Intercept'] + list (X_train.columns),\n",
    "                         'Coefficients' : [logreg.intercept_[0]] +\\\n",
    "                                           list(logreg.coef_[0])\n",
    "                        })\n",
    "# Sort in descending order:\n",
    "coef_df.sort_values (by = ['Coefficients'], ascending = False, inplace = True)\n",
    "# Print table:\n",
    "print (coef_df)\n",
    "#####\n",
    "\n",
    "## Save in a csv file\n",
    "coef_df.to_csv ('LogReg_coefficients_careplans_observations.csv')\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e32a6f",
   "metadata": {},
   "source": [
    "Next steps will include modifying the data to exclude unnecessary features based on their coefficient magnitude and to introduce new features from the *observations* table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f72b1e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
