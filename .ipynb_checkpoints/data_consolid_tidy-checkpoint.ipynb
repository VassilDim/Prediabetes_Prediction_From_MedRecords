{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50936465",
   "metadata": {},
   "source": [
    "# 3. Consolidate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a0f6a",
   "metadata": {},
   "source": [
    "**RECAP:** From **2. Read Data from SQL Database** the following tidied and rearranged for machine learning tables are available in *pickle* format:\n",
    "- patients\n",
    "- allergies\n",
    "- careplans\n",
    "- conditions\n",
    "- immunizations\n",
    "- medications\n",
    "- procedures  \n",
    "  \n",
    "\n",
    "These tables will now be read and merged into a single table to be used for downstream analyses and modeling. In addition, some features (columns) deemed unnecessary will be discarded in order to keep the total dimensions of the table at a manageable size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80bde20",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983c847",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a7377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293de56",
   "metadata": {},
   "source": [
    "## Read tables, merge and save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5ff43",
   "metadata": {},
   "source": [
    "### Patients and allergies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01262f67",
   "metadata": {},
   "source": [
    "Read the *patients* pickle table and the *allergies* pickle table. Merge the 2 based on `patient_id` and keeping every entry from the *patients* table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439bd584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs: 0\n",
      "Dimensions of patients table: (1591423, 5)\n",
      "Dimensions of the allergies 1-hot-encoded table: (165272, 16)\n"
     ]
    }
   ],
   "source": [
    "# Read patients table\n",
    "df = pd.read_pickle ('patients_df.pkl')\n",
    "\n",
    "# Read 1-hot-encoded allergies table\n",
    "allergies_df = pd.read_pickle ('allergies_df_1hot.pkl')\n",
    "\n",
    "# Check for NAs\n",
    "print ('Total NaNs for allergies:', allergies_df.isna().sum().sum())\n",
    "\n",
    "print('Dimensions of patients table:', df.shape)\n",
    "print('Dimensions of the allergies 1-hot-encoded table:', allergies_df.shape)\n",
    "\n",
    "# Merge tables\n",
    "merged_table = pd.merge (\n",
    "    df,\n",
    "    allergies_df,\n",
    "    on = 'patient_id',\n",
    "    how = 'left' # keep all patients from patients table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b404a464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs for merged table: patient_id                        0\n",
      "marital                           0\n",
      "race                              0\n",
      "ethnicity                         0\n",
      "gender                            0\n",
      "Allergy to bee venom        1426499\n",
      "Allergy to dairy product    1426499\n",
      "Allergy to eggs             1426499\n",
      "Allergy to fish             1426499\n",
      "Allergy to grass pollen     1426499\n",
      "Allergy to mould            1426499\n",
      "Allergy to nut              1426499\n",
      "Allergy to peanuts          1426499\n",
      "Allergy to soya             1426499\n",
      "Allergy to tree pollen      1426499\n",
      "Allergy to wheat            1426499\n",
      "Dander (animal) allergy     1426499\n",
      "House dust mite allergy     1426499\n",
      "Latex allergy               1426499\n",
      "Shellfish allergy           1426499\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print ('Total NaNs for merged table:', merged_table.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da6769",
   "metadata": {},
   "source": [
    "Note that most of the patients have no allergies. Therefore, NaNs are introduced upon merging. These NaNs will later be converted to 0 after all tables are consolidated into a single table, in line with the 1-hot-encoding for allergy categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59084b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up:\n",
    "del df, allergies_df\n",
    "\n",
    "# Save temporary merged_table\n",
    "merged_table.to_pickle ('merged_table.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff78604",
   "metadata": {},
   "source": [
    "### Immunizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a914cc6",
   "metadata": {},
   "source": [
    "Read in *immunizations* table and merge to the existing table (containing patients and allergies info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 1-hot-encoded allergies table\n",
    "immunizations = pd.read_pickle ('immunizations_df.pkl')\n",
    "\n",
    "# Check for NAs\n",
    "print ('Total NaNs:', immunizations.isna().sum().sum())\n",
    "\n",
    "print('Dimensions of patients table:', merged_table.shape)\n",
    "print('Dimensions of the immunizations table:', immunizations.shape)\n",
    "\n",
    "# Merge tables\n",
    "df = pd.merge (\n",
    "    merged_table,\n",
    "    immunizations,\n",
    "    on = 'patient_id',\n",
    "    how = 'left' # keep all patients from patients table\n",
    ")\n",
    "\n",
    "df.shape\n",
    "\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Clean up:\n",
    "del merged_table, immunizations\n",
    "\n",
    "# Save temporary merged_table\n",
    "df.to_pickle ('merged_table.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db5220",
   "metadata": {},
   "source": [
    "### Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b209d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle ('merged_table.pkl')\n",
    "\n",
    "medications = pd.read_pickle ('medications_df.pkl')\n",
    "\n",
    "# Check for NAs\n",
    "print ('Total NaNs:', medications.isna().sum().sum())\n",
    "\n",
    "print('Dimensions of merged table:', df.shape)\n",
    "print('Dimensions of the medications table:', medications.shape)\n",
    "\n",
    "medications.drop(columns = ['patient_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd9e5c",
   "metadata": {},
   "source": [
    "Medications table is quite large and the available memory will not allow to read in and merge the entire table. Therefore, merging will be performed in batches of 10 columns at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Determine the number of batches\n",
    "num_batches = int(medications.shape[1] / batch_size) + 1\n",
    "\n",
    "# Iterate through each batch\n",
    "for batch in range(num_batches):\n",
    "    # Calculate the column range for the current batch\n",
    "    start_col = batch * batch_size\n",
    "    end_col = (batch + 1) * batch_size\n",
    "    \n",
    "    # Extract the batch of columns from the right table\n",
    "    batch_columns = medications.iloc[:, start_col:end_col]\n",
    "    \n",
    "    # Merge the left table with the batch of columns from the right table\n",
    "    df = pd.merge(df, \n",
    "                  batch_columns, \n",
    "                  left_on='patient_id', \n",
    "                  right_index=True, \n",
    "                  how = 'left')\n",
    "\n",
    "# Print the merged DataFrame\n",
    "\n",
    "print ('Merged table dimensions:', df.shape)\n",
    "\n",
    "# Clean up memory:\n",
    "del medications\n",
    "\n",
    "# Save merged table\n",
    "df.to_pickle ('merged_table.pkl')\n",
    "\n",
    "# Pause procedures\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede21bcb",
   "metadata": {},
   "source": [
    "### Tidy table and remove unnecessary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0317e50",
   "metadata": {},
   "source": [
    "Due to the large number of features and samples, after loading the data, some columns judged irrelevant will be deleted in order to obtain a more manageable size of the table.  \n",
    "One would not expect the following features to have any effect on prediabetes and some of them are of privacy and ethical concerns:\n",
    "- marital status\n",
    "- race\n",
    "- ethnicity\n",
    "- gender\n",
    "- allergy to bee venom\n",
    "- latex allergy\n",
    "- shellfish allergy\n",
    "- acetaminophen 160 mg\n",
    "- acetaminophen 160 mg oral tablet\n",
    "- ibuprofen 100 mg oral tablet\n",
    "- aspirin 81 mg oral tablet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle ('merged_table.pkl')\n",
    "\n",
    "len(df.columns)\n",
    "\n",
    "# Remove irrelevant columns:\n",
    "cols2remove = ['marital', 'race', 'ethnicity', 'gender', \n",
    "               'Allergy to bee venom', 'Latex allergy', 'Shellfish allergy', \n",
    "               'Acetaminophen 160 MG', 'Acetaminophen 160 MG Oral Tablet', \n",
    "               'Ibuprofen 100 MG Oral Tablet', 'Aspirin 81 MG Oral Tablet']\n",
    "# Drop unnecessary columns\n",
    "df.drop (cols2remove, axis = 1, inplace = True)\n",
    "# Clean up memory\n",
    "del cols2remove\n",
    "# Save file\n",
    "df.to_pickle ('merged_table.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2ba4d",
   "metadata": {},
   "source": [
    "### Conditions (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fec2ec6",
   "metadata": {},
   "source": [
    "*Conditions* is a large table and due to memory limitations, it will be dealt with in batches.  \n",
    "First, all conditions related to diabetes (but NOT prediabetes) will be removed from the table as these are likely correlated with the target variable prediabetes. A new column `prediabetes_bin` will be created to hold the binary encoding for prediabetes: 0 for non-prediabetic and 1 for prediabetic status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in conditions:\n",
    "conditions = pd.read_pickle ('conditions_df.pkl')\n",
    "\n",
    "# Delete all columns that are related to diabetes\n",
    "\n",
    "# Filter and delete columns\n",
    "filtered_cols = conditions.filter(regex=r'^(?!.*Prediabetes.*).*diabetes.*$', axis=1)\n",
    "conditions.drop(filtered_cols.columns, axis=1, inplace=True)\n",
    "\n",
    "del filtered_cols\n",
    "\n",
    "# Remove all rows with only zeroes:\n",
    "conditions = conditions.loc [(conditions != 0).any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for binary pre-diabetes (1 vs 0)\n",
    "conditions['prediabetes_bin'] = np.where(conditions['Prediabetes'] !=0, 1, 0)\n",
    "\n",
    "# Remove the Prediabetes column\n",
    "conditions.drop('Prediabetes', axis = 1, inplace = True)\n",
    "\n",
    "# Save tidy conditions table:\n",
    "conditions.to_pickle ('conditions_tidy.pkl')\n",
    "\n",
    "# Take a break to restart computer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table for conditions:\n",
    "conditions = pd.read_pickle ('conditions_tidy.pkl')\n",
    "# Convert to a csv for downstream manipulation\n",
    "conditions.to_csv ('conditions_tidy.csv')\n",
    "\n",
    "# Clean up memory\n",
    "del conditions\n",
    "\n",
    "# Take a break to restart computer ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881fbb0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7141786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read merged table\n",
    "df = pd.read_pickle ('merged_table.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be4703",
   "metadata": {},
   "source": [
    "Now *conditions* table will be loaded in batches of several columns and merged to the consolidated table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32224f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read columns 0-5\n",
    "start_col = 0\n",
    "end_col = 5\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = range(start_col, end_col)\n",
    "                      )\n",
    "\n",
    "# Merge to df\n",
    "# Merge the left table with the batch of columns from the right table\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f94700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read columns 5-10\n",
    "start_col = 5\n",
    "end_col = 10\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "# Merge to df\n",
    "# Merge the left table with the batch of columns from the right table\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read columns 10-20\n",
    "start_col = 10\n",
    "end_col = 20\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "df.shape\n",
    "\n",
    "## Save table at intermediated stage:\n",
    "df.to_pickle ('merged_table_141.pkl') # Number indicates total number of columns/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read columns 20-30\n",
    "start_col = 20\n",
    "end_col = 30\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "\n",
    "## Save table:\n",
    "df.to_pickle ('merged_table_151.pkl') # Number indicates total number of columns/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read columns 30-35\n",
    "start_col = 30\n",
    "end_col = 35\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "## Save table:\n",
    "df.to_pickle ('merged_table_156.pkl') # Number indicates total number of columns/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read columns 35-37\n",
    "start_col = 35\n",
    "end_col = 37\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "## Save table:\n",
    "df.to_pickle ('merged_table_158.pkl') # Number indicates total number of columns/features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691fe97",
   "metadata": {},
   "source": [
    "#### Clean up data (intermediate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79aeee4",
   "metadata": {},
   "source": [
    "At this stage, data will be cleaned up and tidied in order to reduce dimensions so more feature can be added. As first step, the columns/features containing predominantly 0 will be eliminated as they bring little information to explaining variation of prediabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of zeros per column\n",
    "zeros_per_column = (df == 0).sum()\n",
    "# Sort the result by decreasing number of zeros\n",
    "sorted_zeros_per_column = zeros_per_column.sort_values(ascending=False)\n",
    "\n",
    "# Look at top 20 columns of most zeros\n",
    "sorted_zeros_per_column[:20]\n",
    "\n",
    "# Calculate the number of NaN values per column\n",
    "nan_per_column = df.isna().sum()\n",
    "\n",
    "# Sort the result by decreasing number of NaN values\n",
    "sorted_nan_per_column = nan_per_column.sort_values(ascending=False)\n",
    "sorted_nan_per_column[:20]\n",
    "\n",
    "# Remove 12 columns with most 0s\n",
    "cols2remove = sorted_zeros_per_column[:12].index\n",
    "# Remove the specified columns from the DataFrame\n",
    "df.drop(columns=cols2remove, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2aafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up to liberate memory\n",
    "del cols2remove, nan_per_column, zeros_per_column\n",
    "del sorted_zeros_per_column, sorted_nan_per_column\n",
    "\n",
    "# Save table:\n",
    "df.to_pickle ('merged_table.pkl')\n",
    "\n",
    "# Take a break and restart computer ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e58f7",
   "metadata": {},
   "source": [
    "#### Keep merging with conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908d5fa",
   "metadata": {},
   "source": [
    "Now that 12 columns have been removed, one can proceed with merging more features to the consolidated table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6658110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle ('merged_table.pkl')\n",
    "\n",
    "# read columns 37-44\n",
    "start_col = 37\n",
    "end_col = 44\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "## Save table:\n",
    "df.to_pickle ('merged_table.pkl')\n",
    "\n",
    "# Take a break and restart computer ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c4b08",
   "metadata": {},
   "source": [
    "#### Clean up data again (intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b512b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('merged_table.pkl')\n",
    "\n",
    "# Calculate the number of zeros per column\n",
    "zeros_per_column = (df == 0).sum()\n",
    "# Sort the result by decreasing number of zeros\n",
    "sorted_zeros_per_column = zeros_per_column.sort_values(ascending=False)\n",
    "# Look at top 20 columns of most zeros\n",
    "sorted_zeros_per_column[:20]\n",
    "\n",
    "# Calculate the number of NaN values per column\n",
    "nan_per_column = df.isna().sum()\n",
    "\n",
    "# Sort the result by decreasing number of NaN values\n",
    "sorted_nan_per_column = nan_per_column.sort_values(ascending=False)\n",
    "sorted_nan_per_column[:20]\n",
    "\n",
    "# Drop the column with most 0's\n",
    "df.drop('Fracture of vertebral column without spinal cord injury', axis = 1, inplace = True)\n",
    "\n",
    "# Clean up:\n",
    "del nan_per_column, sorted_nan_per_column, zeros_per_column, sorted_zeros_per_column\n",
    "\n",
    "# Save data\n",
    "df.to_pickle ('merged_table.pkl')\n",
    "\n",
    "# Take a break and restart computer ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e4b25",
   "metadata": {},
   "source": [
    "We will further reduce the dimensions of the data by subsetting the consolidated table so far. The subsetting will be dependent on the `prediabetes_bin` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read consolidated table\n",
    "df = pd.read_pickle ('merged_table.pkl')\n",
    "\n",
    "# Read prediabetes status column only\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', usecols = ['patient_id', 'prediabetes_bin'])\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "# Clean up memory\n",
    "del df_batch\n",
    "# Save consolidated table\n",
    "df.to_pickle ('merged_table.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3c7f8",
   "metadata": {},
   "source": [
    "### Subset the data for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39e3e0",
   "metadata": {},
   "source": [
    "Check how many pre-diabetes entries there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.prediabetes_bin.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354faabb",
   "metadata": {},
   "source": [
    "We have a lot of pre-diabetes patients. We will keep these and subsample the non-prediabetic patients in order to obtained a balanced target variable classes. The entries corresponding to non-prediabetic entries will be subsampled in a random fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f70d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the positive values\n",
    "positive_subset = df[df['prediabetes_bin'] == 1]\n",
    "positive_subset.to_pickle ('merged_data_positive.pkl')\n",
    "del positive_subset\n",
    "\n",
    "# Randomly select non-positive values\n",
    "non_positive_subset = df[df['prediabetes_bin'] == 0].sample(n=412153)\n",
    "non_positive_subset.to_pickle ('merged_data_negative.pkl')\n",
    "del df\n",
    "\n",
    "# Take a break and restart computer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read positive and negative subsets\n",
    "positive_subset = pd.read_pickle ('merged_data_positive.pkl')\n",
    "non_positive_subset = pd.read_pickle ('merged_data_negative.pkl')\n",
    "\n",
    "# Concatenate the subsets\n",
    "df = pd.concat([positive_subset, non_positive_subset])\n",
    "\n",
    "# Shuffle the subset DataFrame\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save table\n",
    "df.to_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "# Clean up memory\n",
    "del positive_subset, non_positive_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f854943",
   "metadata": {},
   "source": [
    "### Conditions (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c4f4a",
   "metadata": {},
   "source": [
    "The rest of the *conditions* table features will be merged to the consolidated table. Note that this is done sequentially and manually rather than using a function or a for loop as the successful outcome for the merging process was rather unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ea787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read columns 44-64\n",
    "start_col = 44\n",
    "end_col = 64\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "# Save table\n",
    "df.to_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "# read columns 64-84\n",
    "start_col = 64\n",
    "end_col = 84\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "# Save table\n",
    "df.to_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "\n",
    "# Take a break and restart computer ...\n",
    "\n",
    "\n",
    "df = pd.read_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "# read columns 84-103\n",
    "start_col = 84\n",
    "end_col = 103\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "# Save table\n",
    "df.to_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "# read columns 103-120\n",
    "start_col = 103\n",
    "end_col = 120\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "\n",
    "# Save table\n",
    "df.to_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "# read columns 120-122\n",
    "start_col = 120\n",
    "end_col = 121\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "# Save table\n",
    "df.to_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "# read columns 122\n",
    "start_col = 122\n",
    "end_col = 122\n",
    "collist = [0] + list(range(start_col, end_col))\n",
    "collist\n",
    "\n",
    "df_batch = pd.read_csv('conditions_tidy.csv', \n",
    "                       usecols = collist\n",
    "                      )\n",
    "\n",
    "df = pd.merge(df,\n",
    "              df_batch,\n",
    "              left_on = 'patient_id',\n",
    "              right_on = 'patient_id',\n",
    "              how = 'inner'\n",
    "             )\n",
    "del df_batch\n",
    "\n",
    "# Save table\n",
    "df.to_pickle ('merged_data_subset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607d734",
   "metadata": {},
   "source": [
    "Check for duplicated columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f261cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated column names\n",
    "duplicated_columns = df.columns.duplicated()\n",
    "\n",
    "# Get the list of duplicated column names\n",
    "duplicated_column_names = df.columns[duplicated_columns].tolist()\n",
    "\n",
    "# Print the duplicated column names\n",
    "print(duplicated_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f6693",
   "metadata": {},
   "source": [
    "No duplication, so the data is almost ready.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca96ff8",
   "metadata": {},
   "source": [
    "### Tidy up data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a9159",
   "metadata": {},
   "source": [
    "As mentioned previously, there are many entries with null values. The handling of these null values will be done using 2 approaches:\n",
    "1. For days elapsed since intervention, the null values will be replaced with the equivalent of 80 years in days (e.g. days since immunizaion).\n",
    "2. For length of intervention (in days), the null values will be replaced with 0 indicating the patient has not undergone the intervention (e.g. days of taking specific medication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in consolidated subsampled data\n",
    "df = pd.read_pickle ('merged_data_subset.pkl')\n",
    "\n",
    "# Display totality of NaN's per columns\n",
    "display(df.isna().sum())\n",
    "\n",
    "display(df.isna().sum()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df947a8",
   "metadata": {},
   "source": [
    "Replace NaNs in immunization columns with the equivalent of 80 years since immunizations (in days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70146e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names to replace for immunizations:\n",
    "cols_immunisations = [\n",
    "    'House dust mite allergy', 'DTaP', 'HPV  quadrivalent', 'Hep A  ped/adol  2 dose',\n",
    "    'Hib (PRP-OMP)', 'IPV', 'Influenza  seasonal  injectable  preservative free',\n",
    "    'Pneumococcal conjugate PCV 13', 'Td (adult) preservative free', 'Tdap',\n",
    "    'meningococcal MCV4P', 'pneumococcal polysaccharide vaccine  23 valent',\n",
    "    'rotavirus  monovalent', 'varicella', 'zoster']\n",
    "\n",
    "for co in cols_immunisations:\n",
    "    print (co)\n",
    "    df[co].fillna(80*365, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e9f23",
   "metadata": {},
   "source": [
    "Replace the rest of the table's NaNs with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a678e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna (0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f163c",
   "metadata": {},
   "source": [
    "Check for NaNs again to make sure everything is numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7eefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f0fb6",
   "metadata": {},
   "source": [
    "No NaN's left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30551f56",
   "metadata": {},
   "source": [
    "Check that all columns are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224309f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the non-numeric column names\n",
    "print (df.select_dtypes(exclude='number').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45f505",
   "metadata": {},
   "source": [
    "The only non-numeric column is `patient_id`. Set it as index and delete the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0b8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.patient_id\n",
    "\n",
    "df.drop (columns = ['patient_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f1961",
   "metadata": {},
   "source": [
    "Table ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652134d",
   "metadata": {},
   "source": [
    "## Save consolidated table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29618af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv ('data_for_model.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc399fa",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
